# -*- coding: utf-8 -*-
"""positional Embediing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15aOvtGGENmgKS-hmyqxIt_PoECCjm6re
"""

! pip3 install tiktoken

import importlib
import tiktoken

from torch.utils.data import Dataset,DataLoader
import torch

class GPT(Dataset):
  def __init__(self,txt,tokenizer,max_length,stride):
    self.input_ids=[]
    self.target_ids=[]
    token_ids=tokenizer.encode(txt,allowed_special={"<|endoftext|>"})
    for i in range(0,len(token_ids)-max_length,stride):
      input_chunk=token_ids[i:i+max_length]
      target_chunk=token_ids[i+1:i+max_length+1]
      self.input_ids.append(torch.tensor(input_chunk))
      self.target_ids.append(torch.tensor(target_chunk))
  def __len__(self):
    return len(self.input_ids)
  def  __getitem__(self,idx):
    return self.input_ids[idx],self.target_ids[idx]

def create_dataloader(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):
  tokenizer=tiktoken.get_encoding("gpt2")
  dataset=GPT(txt,tokenizer,max_length,stride)
  dataloader=DataLoader(
      dataset,
      batch_size=batch_size,
      shuffle=shuffle,
      drop_last=drop_last,
      num_workers=num_workers

  )
  return dataloader

with open("the-verdict.txt","r") as f:
  raw_text=f.read()

vocab_size=50257
output_dim=256
token_embedding=torch.nn.Embedding(vocab_size,output_dim)

max_length=4
dataloader=create_dataloader(
    raw_text,batch_size=8,max_length=max_length,
    stride=max_length,shuffle=False
)
data_iter=iter(dataloader)
inputs,targets=next(data_iter)

print("Token_IDS",inputs)
print(inputs.shape)

token_embeddings=token_embedding(inputs)
print(token_embeddings.shape)

context_length=max_length
pos=torch.nn.Embedding(context_length,output_dim)

pos_embedding=pos(torch.arange(max_length))
pos_embedding

input_embedd=token_embeddings+pos_embedding
print(input_embedd)