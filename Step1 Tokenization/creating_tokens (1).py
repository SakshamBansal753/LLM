# -*- coding: utf-8 -*-
"""Creating tokens

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQb5ew-4N94XV0gGYFPl2uvlN8VHaE10
"""

with open("the-verdict.txt","r") as f:
  raw_text=f.read()
print("Characters",len(raw_text))
print(raw_text[:99])

import re
text="Hello , world.This ,is a test"
result=re.split(r'([,.]|\s)',text)
print(result)

result=[item for item in result if item.strip()]
print(result)

preprocessed=re.split(r'([,:.;?/\"_!()]|--|\s)',raw_text)
preprocessed=[item for item in preprocessed if item.strip()]
print(preprocessed[:30])

print(len(preprocessed))

all_words=sorted(set(preprocessed))
vocab_size=len(all_words)
print(vocab_size)

vocab={token:integer for integer,token in enumerate(all_words)}
for i,item in enumerate(vocab.items()):
  print(item)
  if i>=50:
    break

class SimpleTokenizer:
  def __init__(self,vocab):
    self.str_to_int=vocab
    self.int_to_str={i:s for s,i in vocab.items()}
  def encode(self,text):
    preprocessed=re.split(r'([,.:;?/!\()"]|--|\s)',text)
    preprocessed=[item.strip() for item in preprocessed if item.strip()]
    ids=[self.str_to_int[s] for s in preprocessed]
    return ids
  def decode(self,ids):
    text=" ".join([self.int_to_str[i] for i in ids])
    text=re.sub(r'\s+([,.?!"()\]])',r'\1',text)
    return text

tokenizer=SimpleTokenizer(vocab)
text=""""--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome "obituary" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of "Gisburns" went up.

It was not till:"""
ids=tokenizer.encode(text)
print(ids)

tokenizer.decode(ids)

all_tokens=sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>","<|unk|>"])
vocab={token:integer for integer,token in enumerate(all_tokens)}

class SimpleTokenizerV2:
  def __init__(self,vocab):
    self.str_to_int=vocab
    self.int_to_str={i:s for s,i in vocab.items()}
  def encode(self,text):
    preprocessed=re.split(r'([,.:;?/!\()"]|--|\s)',text)
    preprocessed=[item.strip() for item in preprocessed if item.strip()]
    preprocessed=[item if item in self.str_to_int
                  else "<|unk|>" for item in preprocessed]
    ids=[self.str_to_int[s] for s in preprocessed]
    return ids
  def decode(self,ids):
    text=" ".join([self.int_to_str[i] for i in ids])
    text=re.sub(r'\s+([,.?!"()\]])',r'\1',text)
    return text

tokenizer2=SimpleTokenizerV2(vocab)
text1="Hello ,saksham do you like tea "
text2=" In the sunlight terraces of the palace"
text="<|endoftext|>".join((text1,text2))
print(text)

tokenizer2.encode(text)

tokenizer2.decode(tokenizer2.encode(text))